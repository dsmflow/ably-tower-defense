{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7b8af2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T14:37:58.695922Z",
     "iopub.status.busy": "2024-12-13T14:37:58.695689Z",
     "iopub.status.idle": "2024-12-13T14:44:04.606453Z",
     "shell.execute_reply": "2024-12-13T14:44:04.605682Z"
    },
    "papermill": {
     "duration": 365.915559,
     "end_time": "2024-12-13T14:44:04.608517",
     "exception": false,
     "start_time": "2024-12-13T14:37:58.692958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing micromamba reinstallation to mitigate issues with older images.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import logging\n",
    "import zipfile\n",
    "from typing import List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "import kaggle_evaluation.konwinski_prize_inference_server\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "instance_count = None\n",
    "\n",
    "def get_number_of_instances(num_instances: int) -> None:\n",
    "    global instance_count\n",
    "    instance_count = num_instances\n",
    "    logger.info(f\"Total number of instances: {num_instances}\")\n",
    "\n",
    "def predict(problem_statement: str, repo_archive: io.BytesIO) -> str:\n",
    "    try:\n",
    "        logger.info(\"Starting prediction for new issue\")\n",
    "        \n",
    "        # Unpack the repo archive\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            repo_path = os.path.join(temp_dir, 'repo')\n",
    "            with open(os.path.join(temp_dir, 'repo_archive.tar'), 'wb') as f:\n",
    "                f.write(repo_archive.read())\n",
    "            shutil.unpack_archive(os.path.join(temp_dir, 'repo_archive.tar'), extract_dir=repo_path)\n",
    "\n",
    "            # Analyze the problem statement and generate a patch\n",
    "            patch = generate_patch(repo_path, problem_statement)\n",
    "            logger.info(\"Patch generated successfully\")\n",
    "\n",
    "        return patch\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in predict function: {str(e)}\")\n",
    "        return \"\"  # Return empty string in case of error to skip the issue\n",
    "\n",
    "def generate_patch(repo_path: str, problem_statement: str) -> str:\n",
    "    # Analyze the problem statement\n",
    "    issue_type = analyze_issue(problem_statement)\n",
    "    \n",
    "    # Find relevant files\n",
    "    relevant_files = find_relevant_files(repo_path, issue_type)\n",
    "    \n",
    "    # Generate patch using multi-threading\n",
    "    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "        future_to_file = {executor.submit(process_file, file, issue_type, problem_statement): file for file in relevant_files}\n",
    "        patches = []\n",
    "        for future in as_completed(future_to_file):\n",
    "            file = future_to_file[future]\n",
    "            try:\n",
    "                patch = future.result()\n",
    "                if patch:\n",
    "                    patches.append(patch)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {file}: {str(e)}\")\n",
    "    \n",
    "    return ''.join(patches)\n",
    "\n",
    "def process_file(file: str, issue_type: str, problem_statement: str) -> str:\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        new_content = modify_content(content, issue_type, problem_statement)\n",
    "        \n",
    "        if new_content != content:\n",
    "            return create_diff(file, content, new_content)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {file}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def analyze_issue(problem_statement: str) -> str:\n",
    "    # Simple keyword-based analysis\n",
    "    keywords = {\n",
    "        'bug': ['bug', 'error', 'fix', 'issue', 'problem', 'crash'],\n",
    "        'feature': ['feature', 'add', 'implement', 'new', 'enhance'],\n",
    "        'performance': ['performance', 'slow', 'speed', 'optimize', 'efficient'],\n",
    "    }\n",
    "    \n",
    "    problem_statement = problem_statement.lower()\n",
    "    scores = {category: sum(keyword in problem_statement for keyword in words) \n",
    "              for category, words in keywords.items()}\n",
    "    \n",
    "    if max(scores.values()) == 0:\n",
    "        return 'other'\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "def find_relevant_files(repo_path: str, issue_type: str) -> List[str]:\n",
    "    relevant_files = []\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        content = f.read()\n",
    "                    if issue_type in content.lower():\n",
    "                        relevant_files.append(file_path)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "    return relevant_files[:5]  # Limit to top 5 relevant files\n",
    "\n",
    "def modify_content(content: str, issue_type: str, problem_statement: str) -> str:\n",
    "    if issue_type == 'bug':\n",
    "        # Add error handling\n",
    "        lines = content.split('\\n')\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip().startswith('def '):\n",
    "                new_lines.append(line)\n",
    "                new_lines.append('    try:')\n",
    "                new_lines.append('        ' + '\\n        '.join(lines[lines.index(line)+1:]))\n",
    "                new_lines.append('    except Exception as e:')\n",
    "                new_lines.append('        logger.error(f\"Error: {str(e)}\")')\n",
    "                break\n",
    "            new_lines.append(line)\n",
    "        return '\\n'.join(new_lines)\n",
    "    elif issue_type == 'feature':\n",
    "        # Add TODO comment\n",
    "        return content + f\"\\n\\n# TODO: Implement new feature - {problem_statement}\\n\"\n",
    "    elif issue_type == 'performance':\n",
    "        # Add simple caching\n",
    "        return f\"from functools import lru_cache\\n\\n@lru_cache(maxsize=None)\\n{content}\"\n",
    "    else:\n",
    "        # Add a comment\n",
    "        return f\"# Addressing issue: {problem_statement}\\n{content}\"\n",
    "\n",
    "def create_diff(file_path: str, old_content: str, new_content: str) -> str:\n",
    "    import difflib\n",
    "    diff = difflib.unified_diff(\n",
    "        old_content.splitlines(keepends=True),\n",
    "        new_content.splitlines(keepends=True),\n",
    "        fromfile=file_path,\n",
    "        tofile=file_path\n",
    "    )\n",
    "    return ''.join(diff)\n",
    "\n",
    "def unpack_data():\n",
    "    data_path = '/kaggle/input/konwinski-prize/data.a_zip'\n",
    "    extract_path = '/kaggle/working/data'\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        logger.info(f\"Data extracted to {extract_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error unpacking data: {str(e)}\")\n",
    "\n",
    "inference_server = kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n",
    "    get_number_of_instances,   \n",
    "    predict\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unpack_data()\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                '/kaggle/input/konwinski-prize/',\n",
    "                '/kaggle/working/data/',\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 10462807,
     "sourceId": 84795,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 368.995009,
   "end_time": "2024-12-13T14:44:05.128117",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-13T14:37:56.133108",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
